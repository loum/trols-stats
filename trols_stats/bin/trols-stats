#!/usr/bin/env python

import sys
import os
import argparse
import json
import urllib

import trols_stats.interface
from filer.files import get_directory_files

CONF = os.path.join(os.sep, 'etc', 'trols-stats', 'conf', 'config.conf')
DESCRIPTION = """TROLS Stats Tool"""


def main():
    """Script entry point.

    """
    parser = argparse.ArgumentParser(description=DESCRIPTION)
    parser.add_argument('-c',
                        '--config-file',
                        action='store',
                        dest='config_file')
    # Add sub-command support.
    subparsers = parser.add_subparsers(help='commands')

    # 'source' subcommand.
    source_help = 'Pull data from TROLS endpoint'
    source_parser = subparsers.add_parser('source', help=source_help)
    source_parser.set_defaults(func=source)

    force_help = 'Overwrite cache'
    source_parser.add_argument('-F',
                               '--force',
                               action='store_true',
                               help=force_help,
                               dest='force')

    # 'scrape' subcommand.
    scrape_help = 'Build player game map'
    scrape_parser = subparsers.add_parser('scrape', help=scrape_help)
    scrape_parser.set_defaults(func=scrape)

    dump_help = 'Override file to write out JSON (default "stats.json")'
    scrape_parser.add_argument('-d',
                               '--dump',
                               action='store',
                               default='stats.json',
                               help=dump_help,
                               dest='dump')

    # Prepare the argument list and config.
    args = parser.parse_args()

    config_file = args.config_file
    if args.config_file is None:
        if os.path.exists(CONF):
            config_file = CONF

    if config_file is None:
        sys.exit('Unable to source the default TROLS Stats config.conf')

    conf = trols_stats.Config(config_file)

    args.func(args, conf)


def source(args, conf):
    loader = trols_stats.interface.Loader()

    # Load the competitions.
    #
    # TODO: Archived seasons have a "daytime" query parameter.
    results_url = 'http://trols.org.au/{}/results.php'
    for league, daytime in conf.trols_urls.items():
        option_value = daytime
        if league.lower() == 'nejta':
            daytime = str()

        comps_html = loader.request(results_url.format(league),
                                    {'daytime': daytime})
        comp_name_xpath = (
            '//table/tr/td/select/'
            'option[@value="{}"]/text()'.format(option_value)
        )
        kwargs = {
            'html': comps_html,
            'xpath': comp_name_xpath,
            'tokenise': True,
            'league': league,
        }
        comp_name = trols_stats.Scraper.scrape_competition_name(**kwargs)

        # Restore the daytime.  It's a NEJTA thing :-/
        daytime = option_value

        # Each competition is made of sections.  For example, "BOYS 21".
        # Each section is identified by a code.
        comps_xpath = '//select[@id="section" and @name="section"]/option'
        comps_map = trols_stats.Scraper.scrape_competition_ids(comps_html,
                                                               comps_xpath)

        # Cycle through each competition and get the match codes.
        match_xpath = '//a[contains(@onclick, "open_match")]'
        for code in comps_map.values():
            query_args = {
                'which': 1,
                'style': '',
                'daytime': option_value,
                'section': code,
            }
            matches_html = loader.request(results_url.format(league),
                                          query_args)
            match_codes = trols_stats.Scraper.scrape_match_ids(matches_html,
                                                               match_xpath)

            root_uri = (
                'http://www.trols.org.au/{}/match_popup.php'.format(league)
            )
            for match_code in match_codes:
                request_args = {
                    'matchid': match_code,
                    'seasonid': str(),
                }
                match_uri = '{}?{}'.format(root_uri,
                                           urllib.parse.urlencode(request_args))
                request_kwargs = {
                    'uri': match_uri,
                    'cache_dir':  conf.cache,
                    'force_cache': args.force,
                    'comp_token': comp_name,
                    'match_id': match_code
                }
                loader.request(**request_kwargs)


def scrape(args, conf):
    loader = trols_stats.interface.Loader()

    for html_file in get_directory_files(conf.cache,
                                         file_filter=r'.*.html$'):
        with open(html_file) as _fh:
            loader.build_game_map(_fh.read(), os.path.basename(html_file))

    with open(args.dump, 'w') as games_fh:
        games_fh.write(json.dumps([x() for x in loader.games],
                                  sort_keys=True,
                                  indent=4,
                                  separators=(',', ': ')))

    # Optimise against player ID
    player_id_games = {}
    for game in loader.games:
        token = game.player_id().get('token')
        if player_id_games.get(token) is None:
            player_id_games[token] = []

        player_id_games[token].append(game)

    _db = trols_stats.DBSession(shelve=conf.shelve)
    if _db.connect():
        _db.connection['trols'] = player_id_games
        _db.connection.close()


if __name__ == '__main__':
    main()
